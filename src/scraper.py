import os
import re
import datetime
from pathlib import Path
from bs4 import BeautifulSoup
import requests
from discord_webhook import DiscordWebhook
from dotenv import load_dotenv

class KaldiSaleScraper:
    def __init__(self, html_dir, target_shops=None, webhook_url=None, debug=False):
        """
        åˆæœŸåŒ–
        
        Args:
            html_dir: HTMLãƒ•ã‚¡ã‚¤ãƒ«ãŒä¿å­˜ã•ã‚Œã¦ã„ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹
            target_shops: å¯¾è±¡åº—èˆ—ãƒªã‚¹ãƒˆï¼ˆNoneã®å ´åˆã¯å…¨åº—èˆ—ï¼‰
            webhook_url: Discord Webhook URL
            debug: ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ãƒ•ãƒ©ã‚°
        """
        self.html_dir = Path(html_dir)
        self.target_shops = target_shops
        self.webhook_url = webhook_url
        self.debug = debug
        
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒãªã‘ã‚Œã°ä½œæˆ
        os.makedirs(self.html_dir, exist_ok=True)
    
    def get_kaldi_url(self):
        """
        ç¾åœ¨ã®æ—¥ä»˜ã‚’ä½¿ç”¨ã—ãŸã‚«ãƒ«ãƒ‡ã‚£ã®URLç”Ÿæˆ
        
        Returns:
            ç”Ÿæˆã—ãŸURL
        """
        # ä»Šæ—¥ã®æ—¥ä»˜ã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ (yyyy-MM-dd)
        today = datetime.datetime.now().strftime("%Y-%m-%d")
        # åŸºæœ¬URLã«æ—¥ä»˜ã‚’è¿½åŠ 
        url = f"https://map.kaldi.co.jp/kaldi/articleList?account=kaldi&accmd=1&ftop=1&kkw001={today}"
        return url
        
    def fetch_and_save_html(self, url=None):
        """
        æŒ‡å®šURLã‹ã‚‰HTMLã‚’å–å¾—ã—ã¦ä¿å­˜
        
        Args:
            url: å–å¾—ã™ã‚‹URLï¼ˆNoneã®å ´åˆã¯è‡ªå‹•ç”Ÿæˆï¼‰
            
        Returns:
            ä¿å­˜ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        """
        try:
            # URLãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆã¯è‡ªå‹•ç”Ÿæˆ
            if url is None:
                url = self.get_kaldi_url()
                print(f"è‡ªå‹•ç”Ÿæˆã—ãŸURLã‚’ä½¿ç”¨ã—ã¾ã™: {url}")
                
            # ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ˜ãƒƒãƒ€ãƒ¼ï¼ˆWebã‚µã‚¤ãƒˆã«ã‚ˆã£ã¦ã¯å¿…è¦ï¼‰
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
            
            if self.debug:
                print(f"ãƒªã‚¯ã‚¨ã‚¹ãƒˆé€ä¿¡: {url}")
                print(f"ãƒ˜ãƒƒãƒ€ãƒ¼: {headers}")
            
            # GETãƒªã‚¯ã‚¨ã‚¹ãƒˆé€ä¿¡
            response = requests.get(url, headers=headers, timeout=30)
            response.raise_for_status()  # ã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯
            
            if self.debug:
                print(f"ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰: {response.status_code}")
                print(f"ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ˜ãƒƒãƒ€ãƒ¼: {response.headers}")
                print(f"HTMLé•·ã•: {len(response.text)} ãƒã‚¤ãƒˆ")
            
            # ç¾åœ¨æ—¥æ™‚ã‚’ãƒ•ã‚¡ã‚¤ãƒ«åã«ä½¿ç”¨
            timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"kaldi_sale_{timestamp}.html"
            filepath = self.html_dir / filename
            
            # HTMLã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
            with open(filepath, "w", encoding="utf-8") as f:
                f.write(response.text)
                
            print(f"HTMLã‚’ä¿å­˜ã—ã¾ã—ãŸ: {filepath}")
            return filepath
            
        except requests.exceptions.RequestException as e:
            print(f"HTMLã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            if self.debug:
                import traceback
                traceback.print_exc()
            return None
    
    def parse_html_files(self):
        """ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ã™ã¹ã¦ã®HTMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’è§£æ"""
        sales_info = []
        
        for html_file in self.html_dir.glob("*.html"):
            if self.debug:
                print(f"HTMLãƒ•ã‚¡ã‚¤ãƒ«è§£æ: {html_file}")
                
            with open(html_file, "r", encoding="utf-8") as f:
                content = f.read()
            
            if self.debug:
                print(f"HTMLé•·ã•: {len(content)} ãƒã‚¤ãƒˆ")
                
            soup = BeautifulSoup(content, "html.parser")
            file_sales = self._extract_sales_info(soup)
            
            if self.debug:
                print(f"æŠ½å‡ºã•ã‚ŒãŸã‚»ãƒ¼ãƒ«æƒ…å ±æ•°: {len(file_sales)}")
                if len(file_sales) > 0:
                    print(f"æœ€åˆã®ã‚»ãƒ¼ãƒ«æƒ…å ±ã‚µãƒ³ãƒ—ãƒ«: {file_sales[0]}")
                    
            sales_info.extend(file_sales)
        
        return sales_info
    
    def _extract_sales_info(self, soup):
        """
        BeautifulSoupã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‹ã‚‰ã‚«ãƒ«ãƒ‡ã‚£ã®ã‚»ãƒ¼ãƒ«æƒ…å ±ã‚’æŠ½å‡º
        """
        sales = []
        base_url = "https://map.kaldi.co.jp"
        
        # ãƒ†ãƒ¼ãƒ–ãƒ«è¡Œã§åº—èˆ—ã”ã¨ã®ã‚»ãƒ¼ãƒ«æƒ…å ±ã‚’å–å¾—
        sale_rows = soup.select("table.cz_sp_table tr")
        
        for row in sale_rows:
            # ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ˜ãƒƒãƒ€ãƒ¼ã‚„ç©ºã®è¡Œã¯ç„¡è¦–
            if not row.select_one("td"):
                continue
            
            # åº—èˆ—æƒ…å ±ã‚’å«ã‚€tdã‚’å–å¾—
            shop_cell = row.select_one("td[aria-label='åº—èˆ—åã€ä½æ‰€ãªã©']")
            if not shop_cell:
                continue
                
            # ã‚»ãƒ¼ãƒ«å†…å®¹ã‚’å«ã‚€tdã‚’å–å¾—
            detail_cell = row.select_one("td[aria-label='ã‚»ãƒ¼ãƒ«å†…å®¹']")
            if not detail_cell:
                continue
            
            # åº—èˆ—å
            shop_name_elem = shop_cell.select_one("span.salename a")
            if not shop_name_elem:
                continue
                
            shop_name = shop_name_elem.text.strip()
            
            # å¯¾è±¡åº—èˆ—ã®ã¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            if self.target_shops and shop_name not in self.target_shops:
                continue
                
            # åº—èˆ—URL
            shop_url = base_url + shop_name_elem["href"] if shop_name_elem.has_attr("href") else ""
            
            # åº—èˆ—ä½æ‰€
            shop_address = shop_cell.select_one("span.saleadress").text.strip() if shop_cell.select_one("span.saleadress") else ""
            
            # ã‚»ãƒ¼ãƒ«ã‚¢ã‚¤ã‚³ãƒ³ï¼ˆé–‹å‚¬ä¸­/äºˆå‘Šãªã©ï¼‰- é€šå¸¸ã¨äºˆå‘Šã®ä¸¡æ–¹ã‚’ãƒã‚§ãƒƒã‚¯
            sale_status_elem = shop_cell.select_one("span.saleicon") or shop_cell.select_one("span.saleicon_f")
            sale_status = sale_status_elem.text.strip() if sale_status_elem else ""
            
            # ã‚»ãƒ¼ãƒ«ã‚¿ã‚¤ãƒˆãƒ« - é€šå¸¸ã¨äºˆå‘Šã®ä¸¡æ–¹ã‚’ãƒã‚§ãƒƒã‚¯
            sale_title_elem = shop_cell.select_one("span.saletitle") or shop_cell.select_one("span.saletitle_f")
            sale_title = sale_title_elem.text.strip() if sale_title_elem else ""
            
            # ã‚»ãƒ¼ãƒ«æœŸé–“ï¼ˆé€šå¸¸ã¨äºˆå‘Šã®ä¸¡æ–¹ã‚’ãƒã‚§ãƒƒã‚¯ï¼‰
            sale_date_elem = detail_cell.select_one("p.saledate") or detail_cell.select_one("p.saledate_f")
            sale_date = sale_date_elem.text.strip() if sale_date_elem else ""
            
            # ã‚»ãƒ¼ãƒ«è©³ç´°
            sale_detail = detail_cell.select_one("p.saledetail").text.strip() if detail_cell.select_one("p.saledetail") else ""
            
            # ã‚»ãƒ¼ãƒ«è©³ç´°ã®æ³¨æ„æ›¸ã
            sale_notes = detail_cell.select_one("p.saledetail_notes").text.strip() if detail_cell.select_one("p.saledetail_notes") else ""
            
            # è²©å£²æƒ…å ±ã‚’ã¾ã¨ã‚ã‚‹
            sales.append({
                "shop": shop_name,
                "address": shop_address,
                "status": sale_status,
                "title": sale_title,
                "date": sale_date,
                "detail": sale_detail,
                "notes": sale_notes,
                "url": shop_url
            })
        
        return sales
    
    def format_sale_message(self, sale):
        """ã‚»ãƒ¼ãƒ«æƒ…å ±ã‚’é€šçŸ¥ç”¨ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å¤‰æ›"""
        return f"""
ğŸ”” {sale.get('status', 'æ–°ã‚»ãƒ¼ãƒ«')}!
ğŸ“ {sale.get('shop', 'ä¸æ˜')}
ğŸ¬ {sale.get('address', '')}
ğŸ¯ {sale.get('title', 'ã‚»ãƒ¼ãƒ«è©³ç´°ä¸æ˜')}
ğŸ“… {sale.get('date', 'æ—¥ä»˜ä¸æ˜')}
ğŸ’° {sale.get('detail', '')}
ğŸ”— {sale.get('url', '#')}
"""

    def save_to_text_file(self, sales_info, output_file="sales_output.txt"):
        """ã‚»ãƒ¼ãƒ«æƒ…å ±ã‚’ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        if not sales_info:
            return False
        
        with open(output_file, "w", encoding="utf-8") as f:
            for sale in sales_info:
                message = self.format_sale_message(sale)
                f.write(message)
                f.write("\n" + "-"*40 + "\n")  # åŒºåˆ‡ã‚Šç·š
        
        return True
        
    def notify_discord(self, sales_info):
        """Discord Webhookã‚’ä½¿ç”¨ã—ã¦ã‚»ãƒ¼ãƒ«æƒ…å ±ã‚’é€šçŸ¥"""
        if not self.webhook_url or not sales_info:
            if self.debug:
                if not self.webhook_url:
                    print("Discordé€šçŸ¥: Webhook URLãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
                if not sales_info:
                    print("Discordé€šçŸ¥: é€šçŸ¥ã™ã‚‹ã‚»ãƒ¼ãƒ«æƒ…å ±ãŒã‚ã‚Šã¾ã›ã‚“")
            return False
        
        if self.debug:
            print(f"Discordé€šçŸ¥: {len(sales_info)}ä»¶ã®ã‚»ãƒ¼ãƒ«æƒ…å ±ã‚’é€ä¿¡ã—ã¾ã™")
            # Webhookã®æœ€åˆã®æ•°æ–‡å­—ã‚’è¡¨ç¤ºï¼ˆã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã®ãŸã‚å…¨éƒ¨ã¯è¡¨ç¤ºã—ãªã„ï¼‰
            webhook_prefix = self.webhook_url[:30] + "..." if len(self.webhook_url) > 30 else self.webhook_url
            print(f"Discord Webhook URL: {webhook_prefix}")
        
        for i, sale in enumerate(sales_info):
            message = self.format_sale_message(sale)
            
            if self.debug:
                print(f"Discordé€šçŸ¥ {i+1}/{len(sales_info)}: {sale.get('shop')} - {sale.get('title')}")
                
            webhook = DiscordWebhook(url=self.webhook_url, content=message)
            response = webhook.execute()
            
            if self.debug:
                print(f"Discordé€šçŸ¥ãƒ¬ã‚¹ãƒãƒ³ã‚¹: ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ {response.status_code}")
            
            # APIåˆ¶é™ã‚’é¿ã‘ã‚‹ãŸã‚å°‘ã—å¾…æ©Ÿ
            if i < len(sales_info) - 1:
                import time
                time.sleep(1)
        
        return True

def main():
    import argparse
    
    # .envãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
    load_dotenv()
    
    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã®è¨­å®š
    parser = argparse.ArgumentParser(description='ã‚«ãƒ«ãƒ‡ã‚£ã‚»ãƒ¼ãƒ«æƒ…å ±å–å¾—ãƒ„ãƒ¼ãƒ«')
    parser.add_argument('--url', type=str, help='ã‚»ãƒ¼ãƒ«æƒ…å ±ã‚’å–å¾—ã™ã‚‹URLï¼ˆæŒ‡å®šã—ãªã„å ´åˆã¯ç¾åœ¨ã®æ—¥ä»˜ã§è‡ªå‹•ç”Ÿæˆï¼‰')
    parser.add_argument('--notify', action='store_true', help='Discordã«é€šçŸ¥ã™ã‚‹')
    parser.add_argument('--output', type=str, help='å‡ºåŠ›ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆ.envãƒ•ã‚¡ã‚¤ãƒ«ã®è¨­å®šã‚’ä¸Šæ›¸ãã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: sales_output.txtï¼‰')
    parser.add_argument('--fetch-only', action='store_true', help='HTMLã®å–å¾—ã®ã¿ã‚’è¡Œã†')
    parser.add_argument('--no-fetch', action='store_true', help='HTMLã®å–å¾—ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¦æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è§£æ')
    parser.add_argument('--shops', type=str, help='å¯¾è±¡åº—èˆ—ã®ãƒªã‚¹ãƒˆï¼ˆã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šï¼‰')
    parser.add_argument('--all-shops', action='store_true', help='å…¨åº—èˆ—ã‚’å¯¾è±¡ã«ã™ã‚‹')
    parser.add_argument('--discord-webhook', type=str, help='Discord Webhook URLï¼ˆ.envãƒ•ã‚¡ã‚¤ãƒ«ã®è¨­å®šã‚’ä¸Šæ›¸ãï¼‰')
    parser.add_argument('--debug', action='store_true', help='ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ï¼ˆè©³ç´°æƒ…å ±ã‚’è¡¨ç¤ºï¼‰')
    args = parser.parse_args()
    
    # Discord Webhook URLã‚’å–å¾—ï¼ˆå„ªå…ˆé †ä½: ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•° > ç’°å¢ƒå¤‰æ•°ï¼‰
    webhook_url = None
    if args.notify:
        webhook_url = args.discord_webhook or os.environ.get("DISCORD_WEBHOOK_URL")
    
    # å¯¾è±¡åº—èˆ—ãƒªã‚¹ãƒˆï¼ˆå„ªå…ˆé †ä½: ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ã‚ªãƒ—ã‚·ãƒ§ãƒ³ > ç’°å¢ƒå¤‰æ•° > ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ï¼‰
    if args.all_shops:
        target_shops = None  # ã™ã¹ã¦ã®åº—èˆ—ã‚’å¯¾è±¡
    elif args.shops:
        target_shops = [shop.strip() for shop in args.shops.split(',')]
    elif os.environ.get("TARGET_SHOPS"):
        target_shops = [shop.strip() for shop in os.environ.get("TARGET_SHOPS").split(',')]
    else:
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®å¯¾è±¡åº—èˆ—ãƒªã‚¹ãƒˆ
        target_shops = ["æ± è¢‹åº—", "æ¸‹è°·åº—", "æ–°å®¿åº—", "ç«‹å·è‹¥è‘‰ã‚±ãƒ¤ã‚­ãƒ¢ãƒ¼ãƒ«åº—"]
    
    # ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ã®è¨­å®šï¼ˆã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ã‚ªãƒ—ã‚·ãƒ§ãƒ³ > ç’°å¢ƒå¤‰æ•°ï¼‰
    debug_mode = args.debug or os.environ.get("DEBUG") == "1"
    
    # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
    scraper = KaldiSaleScraper(
        html_dir="./data",
        target_shops=target_shops,
        webhook_url=webhook_url,
        debug=debug_mode
    )
    
    # no-fetchã‚ªãƒ—ã‚·ãƒ§ãƒ³ãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆã¯HTMLã‚’å–å¾—
    if not args.no_fetch:
        html_path = scraper.fetch_and_save_html(args.url)  # å¼•æ•°ãªã—ã®å ´åˆã¯è‡ªå‹•ç”ŸæˆURLã‚’ä½¿ç”¨
        if not html_path:
            print("HTMLã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
            return
        
        # fetch-onlyã‚ªãƒ—ã‚·ãƒ§ãƒ³ãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ã“ã“ã§çµ‚äº†
        if args.fetch_only:
            return
    
    # ã‚»ãƒ¼ãƒ«æƒ…å ±ã‚’æŠ½å‡º
    sales_info = scraper.parse_html_files()
    
    # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«åã®æ±ºå®šï¼ˆå„ªå…ˆé †ä½: ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ã‚ªãƒ—ã‚·ãƒ§ãƒ³ > ç’°å¢ƒå¤‰æ•° > ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ï¼‰
    output_file = args.output or os.environ.get("OUTPUT_FILE") or "sales_output.txt"
    
    if sales_info:
        # ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        scraper.save_to_text_file(sales_info, output_file)
        print(f"{len(sales_info)}ä»¶ã®ã‚»ãƒ¼ãƒ«æƒ…å ±ã‚’{output_file}ã«ä¿å­˜ã—ã¾ã—ãŸ")
        
        # Discordé€šçŸ¥ï¼ˆ--notifyã‚ªãƒ—ã‚·ãƒ§ãƒ³ãŒã‚ã‚‹å ´åˆã®ã¿ï¼‰
        if args.notify and webhook_url:
            scraper.notify_discord(sales_info)
            print(f"{len(sales_info)}ä»¶ã®ã‚»ãƒ¼ãƒ«æƒ…å ±ã‚’Discordã«é€šçŸ¥ã—ã¾ã—ãŸ")
    else:
        print("ã‚»ãƒ¼ãƒ«æƒ…å ±ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")

if __name__ == "__main__":
    main()